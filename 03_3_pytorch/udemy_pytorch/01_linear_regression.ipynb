{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with PyTorch\n",
    "\n",
    "- x : independent variable\n",
    "- y : depdent variable\n",
    "\n",
    "$$ y = \\alpha x + \\beta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simple example\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.randn(n)\n",
    "\n",
    "colors = np.random.rand(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aim: minimize the distance between the points and the line\n",
    "\n",
    "- adjusting (coefficient : $\\alpha$) or (bias : $\\beta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coefficient = 2\n",
    "- bias = 1\n",
    "- equation $y = 2x+ 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = [i for i in range(11)]\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to numpy\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D shape\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = [2 * i + 1 for i in x_values]\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model y=2x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instantiate loss\n",
    "- MSE loss (Mean Squared Error)\n",
    "- $MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)$\n",
    "    - $\\hat y$: prediction\n",
    "    - $y$: true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "- Simplified equation\n",
    "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
    "        - $\\theta$: parameters (our variables)\n",
    "        - $\\eta$: learning rate (how fast we want to learn)\n",
    "        - $\\nabla_\\theta$: parameters' gradients\n",
    "- Even simplier equation\n",
    "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
    "        - parameters: $\\alpha$ and $\\beta$ in $ y = \\alpha x + \\beta$\n",
    "        - desired parameters: $\\alpha = 2$ and $\\beta = 1$ in $ y = 2x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "- 1 epoch = going through the whole x_train data once\n",
    "\n",
    "\n",
    "1. convert inputs/labels to variables\n",
    "2. clear gradient buffets\n",
    "3. get output\n",
    "4. get loss\n",
    "5. get gradients w.r.t parameters\n",
    "6. update parameters with gradients\n",
    "7. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 260.6395263671875\n",
      "epoch 2, loss 21.328079223632812\n",
      "epoch 3, loss 1.8074325323104858\n",
      "epoch 4, loss 0.21443681418895721\n",
      "epoch 5, loss 0.083753302693367\n",
      "epoch 6, loss 0.07235391438007355\n",
      "epoch 7, loss 0.07069242745637894\n",
      "epoch 8, loss 0.06983327865600586\n",
      "epoch 9, loss 0.06904792040586472\n",
      "epoch 10, loss 0.06827633827924728\n",
      "epoch 11, loss 0.06751392781734467\n",
      "epoch 12, loss 0.06675992906093597\n",
      "epoch 13, loss 0.06601448357105255\n",
      "epoch 14, loss 0.06527730077505112\n",
      "epoch 15, loss 0.06454837322235107\n",
      "epoch 16, loss 0.06382744014263153\n",
      "epoch 17, loss 0.06311486661434174\n",
      "epoch 18, loss 0.06240992620587349\n",
      "epoch 19, loss 0.06171305105090141\n",
      "epoch 20, loss 0.06102386862039566\n",
      "epoch 21, loss 0.06034243479371071\n",
      "epoch 22, loss 0.05966861918568611\n",
      "epoch 23, loss 0.05900230258703232\n",
      "epoch 24, loss 0.058343470096588135\n",
      "epoch 25, loss 0.057692043483257294\n",
      "epoch 26, loss 0.057047802954912186\n",
      "epoch 27, loss 0.05641062930226326\n",
      "epoch 28, loss 0.055780790746212006\n",
      "epoch 29, loss 0.05515776202082634\n",
      "epoch 30, loss 0.05454188212752342\n",
      "epoch 31, loss 0.05393289402127266\n",
      "epoch 32, loss 0.05333051458001137\n",
      "epoch 33, loss 0.05273496359586716\n",
      "epoch 34, loss 0.052146125584840775\n",
      "epoch 35, loss 0.051563795655965805\n",
      "epoch 36, loss 0.05098803713917732\n",
      "epoch 37, loss 0.05041860044002533\n",
      "epoch 38, loss 0.0498555563390255\n",
      "epoch 39, loss 0.04929880052804947\n",
      "epoch 40, loss 0.048748407512903214\n",
      "epoch 41, loss 0.048204030841588974\n",
      "epoch 42, loss 0.047665707767009735\n",
      "epoch 43, loss 0.04713347926735878\n",
      "epoch 44, loss 0.04660715535283089\n",
      "epoch 45, loss 0.046086572110652924\n",
      "epoch 46, loss 0.04557197540998459\n",
      "epoch 47, loss 0.04506317153573036\n",
      "epoch 48, loss 0.04455987364053726\n",
      "epoch 49, loss 0.044062428176403046\n",
      "epoch 50, loss 0.043570272624492645\n",
      "epoch 51, loss 0.043083857744932175\n",
      "epoch 52, loss 0.0426025390625\n",
      "epoch 53, loss 0.04212690889835358\n",
      "epoch 54, loss 0.04165646433830261\n",
      "epoch 55, loss 0.04119130223989487\n",
      "epoch 56, loss 0.04073123633861542\n",
      "epoch 57, loss 0.04027656838297844\n",
      "epoch 58, loss 0.03982679545879364\n",
      "epoch 59, loss 0.03938206657767296\n",
      "epoch 60, loss 0.038942161947488785\n",
      "epoch 61, loss 0.03850734978914261\n",
      "epoch 62, loss 0.038077421486377716\n",
      "epoch 63, loss 0.03765213489532471\n",
      "epoch 64, loss 0.0372316800057888\n",
      "epoch 65, loss 0.03681586682796478\n",
      "epoch 66, loss 0.03640483692288399\n",
      "epoch 67, loss 0.035998307168483734\n",
      "epoch 68, loss 0.03559635579586029\n",
      "epoch 69, loss 0.03519877791404724\n",
      "epoch 70, loss 0.034805696457624435\n",
      "epoch 71, loss 0.03441710025072098\n",
      "epoch 72, loss 0.03403270244598389\n",
      "epoch 73, loss 0.033652614802122116\n",
      "epoch 74, loss 0.03327690064907074\n",
      "epoch 75, loss 0.03290537744760513\n",
      "epoch 76, loss 0.03253785893321037\n",
      "epoch 77, loss 0.0321744829416275\n",
      "epoch 78, loss 0.03181523457169533\n",
      "epoch 79, loss 0.03146001324057579\n",
      "epoch 80, loss 0.031108684837818146\n",
      "epoch 81, loss 0.03076121397316456\n",
      "epoch 82, loss 0.03041766583919525\n",
      "epoch 83, loss 0.03007807768881321\n",
      "epoch 84, loss 0.02974224090576172\n",
      "epoch 85, loss 0.029410164803266525\n",
      "epoch 86, loss 0.029081596061587334\n",
      "epoch 87, loss 0.028756937012076378\n",
      "epoch 88, loss 0.028435776010155678\n",
      "epoch 89, loss 0.02811828814446926\n",
      "epoch 90, loss 0.027804234996438026\n",
      "epoch 91, loss 0.027493828907608986\n",
      "epoch 92, loss 0.027186725288629532\n",
      "epoch 93, loss 0.026883233338594437\n",
      "epoch 94, loss 0.02658292092382908\n",
      "epoch 95, loss 0.0262860506772995\n",
      "epoch 96, loss 0.025992585346102715\n",
      "epoch 97, loss 0.025702308863401413\n",
      "epoch 98, loss 0.02541527710855007\n",
      "epoch 99, loss 0.025131525471806526\n",
      "epoch 100, loss 0.02485082298517227\n",
      "Time 0:00:00.036790"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for e in range(epochs):\n",
    "    e += 1\n",
    "    \n",
    "    # convert inputs/labels to variables\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward & get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(e, loss.data.item()))\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7067548],\n",
       "       [ 2.7489848],\n",
       "       [ 4.791215 ],\n",
       "       [ 6.833445 ],\n",
       "       [ 8.875675 ],\n",
       "       [10.917906 ],\n",
       "       [12.960135 ],\n",
       "       [15.002365 ],\n",
       "       [17.044596 ],\n",
       "       [19.086826 ],\n",
       "       [21.129057 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl0XOV5+PHvM9pGu0e7bVm2wbbkBXlBGDssNmDWEAgONCEloYlTJzmhaXrqUNL+WvJL0x5y4pBwfqRJ3YQALYFsgpCWgE1I6oQAxgbb2JJAtrG172PtI2lmnt8fM1ZkWUZCI2k0M8/nHB3Nvfe99z4jy4/eee+9zyuqijHGmNjhCHcAxhhjZpYlfmOMiTGW+I0xJsZY4jfGmBhjid8YY2KMJX5jjIkxlviNMSbGWOI3xpgYY4nfGGNiTHy4AxhLTk6OLlq0KNxhGGNMxDhw4ECbquZOpO2sTPyLFi1i//794Q7DGGMihoicmmhbG+oxxpgYY4nfGGNijCV+Y4yJMbNyjH8sQ0ND1NXV4fF4wh1KxHM6nRQWFpKQkBDuUIwxYRAxib+uro709HQWLVqEiIQ7nIilqrS3t1NXV8fixYvDHY4xJgwiZqjH4/GQnZ1tST9EIkJ2drZ9cjImhkVMjx+wpD9F7OdozOxyuOkw5VXl1HTWUJRZxNaSrZQWlE7b+SKmx2+MMdHocNNhdr6yk6bOHgozCnH3u9n5yk4ONx2etnNa4p+A9vZ21qxZw5o1aygoKGD+/PnDy4ODg9N23ssvv5yDBw++Z5sHH3zQhm2MiWA/q3iawd6lNLYso6s3BVeyC5fTRXlV+bSdM6KGet6PqfzolJ2dPZyAv/rVr5KWlsaOHTvOaqOqqCoOx8z+LX3wwQf59Kc/jdPpnNHzGmNC925bL6+8nUBqfDa5c3pITxkAINOZSU1nzbSdNyp7/Gc+Orn73dP60enYsWOsWLGCP//zP2flypXU1tYyZ86c4e1PPfUUn/nMZwBobm5m69atlJWVsX79el599dVzjtfX18cdd9zB8uXL+chHPnJWT3779u2UlZWxcuVKvva1rwHw7W9/m5aWFq644gq2bNly3nbGmNmnqqmLZ96sJztlDvk5xynM7SLOoQB0ejopyiyatnNHZY+/vKocl9OFK9kFMPy9vKp8yi+YVFVV8fjjj1NWVobX6z1vuy9+8Yvce++9bNiwgZMnT3LzzTdz5MiRs9o8/PDDuFwuKisrefPNNykrKxve9sADD5CVlYXX6+Wqq67i9ttv52/+5m/41re+xe9///vhPzhjtVuxYsWUvmdjzOSoKp4hP8mJcVyYm8aVy3LYvPIyvv3aq7j7XWQ6M+n0dOL2uNm2dtu0xRGVib+ms4bCjMKz1k3XR6cLL7zwrAR9Pi+++CJvv/328LLb7aa/v5/k5OThdXv37uXee+8FYO3ataxcuXJ425NPPskPf/hDvF4vDQ0NVFRUjJnQJ9rOGDOzega8vFTVQnvPAHdtWEhCnIOLF2YBWezYuOOsoelta7dN6109UZn4izKLcPe7h3v6MH0fnVJTU4dfOxwOVHV4eeRQjaqyb98+EhMT3/c5qqureeihh9i3bx9z5szhrrvuGvOC7kTbGWNmjqpytKGLvdWt+HzKxguziRt1S3VpQem0JvrRonKMf2vJVtweN+5+N3714+534/a42VqydVrP63A4cLlcVFdX4/f7efrpp4e3bdmyhe9+97vDy2PdrXPllVfy4x//GIBDhw5x9OhRALq6ukhPTycjI4PGxkZeeOGF4X3S09Pp7u4et50xZuZ5hnyUv1HPnopmctKSuGvDQsoWZeFwhPdZmqjs8ZcWlM74R6czvvGNb3D99deTl5fHxRdfzMBA4Cr9d7/7XT7/+c/zox/9aHj8feQfAoB77rmHu+++m+XLl7Ny5UrWrl0LwLp161ixYgUlJSUsXLiQyy67bHif7du3s2XLFhYsWMCePXvO284YM/MS4xw4HHDN8jwump85ax6elJFDE2M2EFkAPA7kAwrsUtWHRCQL+AmwCDgJ/JmqusfY/27g/wQXv66qj40XVFlZmY6eiKWyspLly5ePt6uZIPt5GjM92nsG+MOxNq5dkU9KYjyqOiMJX0QOqOr4FxyZ2FCPF/hbVV0BbAC+ICIrgPuA36jqUuA3weXRgWQB9wOXAuuB+0XENbqdMcZEOp9fee1EO0+8VkNjp4eO3sDDnbOllz/SuEM9qtoINAZfd4tIJTAfuBXYHGz2GPA74O9G7X49sEdVOwBEZA9wA/DkFMRujDGzQnOXh90VzbR1D1BckM7m4lxSEmfvSPr7ikxEFgFrgdeA/OAfBYAmAkNBo80Hakcs1wXXjXXs7cB2gKKi6XtwwRhjptobp9x4Bn3csmYeF+amhTuccU048YtIGvAL4Euq2jXy44uqqoi898WCcajqLmAXBMb4QzmWMcZMt9qOPlKT4slKTWRzcR4i4EyIC3dYEzKh2zlFJIFA0n9CVc9UDmoWkbnB7XOBljF2rQcWjFguDK4zxpiINOD18ZvKZn5+oI5XT7QDkJwYFzFJHyaQ+CXQtf8hUKmqD47Y9Cxwd/D13cAvx9j9BeA6EXEFL+peF1xnjDER5922Xv7zlVO8Vd/JuoUutiwfa4R79ptIj/8y4BPA1SJyMPh1E/AAcK2IVANbgsuISJmI/AAgeFH3n4HXg19fO3OhNxLFxcWxZs0aVq1axR133EFfX9+kj/W73/2Om2++GYBnn32WBx544LxtT58+zb/927+973N89atfZefOnZOO0RjzJ2eKqiXFO/joJQvYtCyXxPjIfAZ23KhV9Q+qKqpaqqprgl/PqWq7ql6jqktVdcuZhK6q+1X1MyP2f0RVlwS/fjSdb2a6JScnc/DgQY4cOUJiYiLf//73z9quqvj9/vd93FtuuYX77jvnbthhk038xpjQqCp9g4Hii4Giarl8/NKFzM1MHmfP2S0y/1zNAldccQXHjh3j5MmTFBcX88lPfpJVq1ZRW1vL7t272bhxI+vWreOOO+6gp6cHgOeff56SkhLWrVtHefmfJll49NFHueeee4BA+ebbbruN1atXs3r1av74xz9y3333cfz4cdasWcOXv/xlAL75zW9yySWXUFpayv333z98rH/5l39h2bJlXH755WcVhTPGvD89A16ePdTAT16vZcjnDxZVcxEX5nILU2H23mg6jp/trz1n3bL8dFYvmMOQz88zb557DXnFvAxWzsukf9DHfx9uOGvbHWULzml/Pl6vl1//+tfccMMNQKA42mOPPcaGDRtoa2vj61//Oi+++CKpqal84xvf4MEHH+Tee+/lL//yL3nppZdYsmQJH/3oR8c89he/+EU2bdrE008/jc/no6enhwceeIAjR44M1/fZvXs31dXV7Nu3D1XllltuYe/evaSmpvLUU09x8OBBvF4v69at4+KLL57w+zIm1h1uOswvKsupaOjCN7CMFTmruG31inOKqkW6iE384dDf38+aNWuAQI9/27ZtNDQ0sHDhQjZs2ADAq6++SkVFxXCdnMHBQTZu3EhVVRWLFy9m6dKlANx1113s2rXrnHO89NJLPP7440DgmkJmZiZu99mVMHbv3s3u3buHa/n09PRQXV1Nd3c3t912GykpKUBgCMkYMzGHmw7zjZe/TV9XCT7vXOLiT3N84FESkr6Iw5EV7vCmVMQm/vfqoSfEOd5ze3Ji3Pvq4Q/vFxzjH21kaWZV5dprr+XJJ89+OHm8uXPfD1XlK1/5Cp/97GfPWv+d73xnys5hTKwpryonOzkDx0AymVmdZGf0c9qTOi0TOIWbjfFPsQ0bNvDyyy9z7NgxAHp7e3nnnXcoKSnh5MmTHD9+HOCcPwxnXHPNNXzve98DwOfz0dnZeVbpZYDrr7+eRx55ZPjaQX19PS0tLVx55ZU888wz9Pf3093dza9+9avpfKvGRIW2ngGefrOOEx31zEnO5IK5HeRk9iEy/XPfhosl/imWm5vLo48+yp133klpaenwMI/T6WTXrl188IMfZN26deTl5Y25/0MPPcRvf/tbLrroIi6++GIqKirIzs7msssuY9WqVXz5y1/muuuu4+Mf/zgbN27koosu4vbbb6e7u5t169bx0Y9+lNWrV3PjjTdyySWXzPC7NyZy+PzKK8fb+fFrNTR3DZCXvJhOTycjh/One+7bcBm3LHM4WFnm6Wc/TxPLmjo97Klooq1nkJKCdDYX51HdcZSdr+zE5Tx77tsdG3dExFDPVJdlNsaYqPJmjZsBr59b18zjxovmkpwYNzyBkyvZRV1XHa5kV8Qk/fcrYi/uGmPM+1Hb0UdKYhzZaUlsLs7D4YCk+LPr68z03LfhElE9/tk4LBWJ7OdoYolnyMeLFYGiavveDVSMSU6MOyfpx5KI6fE7nU7a29vJzs6elTPaRApVpb29HafTGe5QjJl2x1t7eKmyhd5BLxcvdLHxwuxwhzQrREziLywspK6ujtbW1nCHEvGcTieFhYXhDsOYaVXZ2MXzR5rISU/iQ6vnUZBpnZ0zIibxJyQksHjx4nCHYYyZxQJF1XykJsWzJC+NTcW5rC6cExX1daZSRI3xG2PM+XR5hs4pqrauKDqKqk21iOnxG2PMWFSVt+o7+X11G6rKB5bkRF1Rtak2buIXkUeAm4EWVV0VXPcToDjYZA5wWlXXjLHvSaAb8AHeiT5cYIwxE+EZ8vGrQw3Uufspykphy/J8MlMSwh3WrDeRHv+jwMPA42dWqOpwTWER+RbQ+R77X6WqbZMN0Bhjzicp3kFivINrV+Szcl6G3fE3QeMmflXdKyKLxtoWnI/3z4CrpzYsY4wZW2v3AL+vbuX6lQWkJsVz65r54Q4p4oQ6xn8F0Kyq1efZrsBuEVHg31X13AL0QSKyHdgOUFQUfUWRjDGh8fr87DvZwevvunEmODjdP0Rqkl2mnIxQf2p3AmPXFw64XFXrRSQP2CMiVaq6d6yGwT8KuyBQpC3EuIwxUaSxs589Fc209wyyfG46m5blkZwYu0/ehmrSiV9E4oGtwHnn9lPV+uD3FhF5GlgPjJn4jTEGAjNhlVeVU9NZQ1FmEVtLtlLfmsug18+H185ncU7q+Acx7ymU+/i3AFWqWjfWRhFJFZH0M6+B64AjIZzPGBPlDjcdZucrO3H3u8mMv5Cmzh52vrKTHFczn9i40JL+FBk38YvIk8ArQLGI1InItuCmjzFqmEdE5onIc8HFfOAPInII2Af8j6o+P3WhG2OiTXlVORmJ2XR3LeZEQy6e/gW4nC7+59gzMV1UbapN5K6eO8+z/i/GWNcA3BR8fQJYHWJ8xpgYUtnkxttfjM8XT76rh4KsbpDonP4wnOySuDFmVqhs7MLTvRJ1dFJSOEiKcwgAd390Tn8YTlarxxgTNqpK74AXgCV5adxVtpZM1xEGtAW/+nH3u3F73Gwt2RrmSKOLJX5jTFh0eYb45cFAUbVBb6Co2h1rL+HLH/jbmJj+MJxsqMcYM6NUlcN1nfzhWKCSywcuzCZ+RAXNWJn+MJws8RtjZoxnyMezhxqod/ezMDuFa5bnk5lsRdVmmiV+Y8yMSYp3kBTv4LqV+ayYa0XVwsXG+I0x06ql28MvDtTRO+BFRLh1zXxWzsu0pB9G1uM3xkwLr8/Pa+92sP+km+REK6o2m9i/gjFmytWf7ufFimY6egdZMS+DTctycSbYk7ezhSV+Y8yUO1x7miGfn9vWzmeR1deZdSzxG2OmxKn2XtKS4slOS+KqkjxEsPo6s5Rd3DXGhMQz5OOFo02Uv1HP6yc7AHAmxFnSn8Wsx2+MmbRjLd28VNVC/6Cf9YuzuHRxVrhDMhNgid8YMymVjV08f6SJvIwkPrw2n7x0Z7hDMhNkid8YM2GqSu+gj7SkeJbkpXFVSR4Xzc8kzmH35EeSiUzE8oiItIjIkRHrvioi9SJyMPh103n2vUFE3haRYyJy31QGboyZWZ39Qzz9Zj0/HVFUbc2COZb0I9BEevyPAg8Dj49a/21V3Xm+nUQkDvgucC1QB7wuIs+qasUkYzXGzKAzc9+eOl1DCstxOTYyN72Ay5fkkBBnyT6SjdvjV9W9QMckjr0eOKaqJ1R1EHgKuHUSxzHGzLAzc9+29XTi6V7LscYkXm9+gbWLe1i9YI6VW4hwodzOeY+IHA4OBbnG2D4fqB2xXBdcZ4yZ5cqrynE5XWSnZpIYrxTP76d4fhe7Tz4T7tDMFJhs4v8ecCGwBmgEvhVqICKyXUT2i8j+1tbWUA9njJmkli4Pr1VDSrwLEVg8t4OsjH7mJNvct9FiUolfVZtV1aeqfuA/CAzrjFYPLBixXBhcd75j7lLVMlUty83NnUxYxpgQDPn8/KG6jSf31ZIaX0B7T99Z2zs9NvdttJhU4heRuSMWbwOOjNHsdWCpiCwWkUTgY8CzkzmfMWZ61Z/u54lXT/H6yQ6Wz03n767dgIcm3P1um/s2Co17V4+IPAlsBnJEpA64H9gsImsABU4Cnw22nQf8QFVvUlWviNwDvADEAY+o6tFpeRfGmJC8VXcan8JH1hVSlJ0CFLAjfgflVeXUdNZQlFnEtrXbbErEKCGqGu4YzlFWVqb79+8PdxjGRLV323pJd8aTk5aEZ8iHQ4TEeCvfFalE5ICqlk2krf0rGxNj+gd9PH+kiWferGf/iKJqlvRjh5VsMCZGqCrVLT38tqoFz5CfSy/IYv0iK6oWiyzxGxMjKhu7eeFoE/kZTrauyyc3PSncIZkwscRvTBRTVXoGvKQ7E1iWn4bXn8eqeZk4rL5OTLNBPWOiVGffEOVv1PPT/XUMev3ExzkoLZxjSd9Yj9+YaOP3KwfrTvPHY22ICFcstaJq5myW+I2JIv2DPn55sJ7GTg+Lc1K5enkeGc6EcIdlZhlL/MZEEWeCg9SkeG5YVUBJQbpV0TRjsjF+YyJcU6eHn+6vpWfAi4jwodXzWD43w5K+OS/r8RsToYZ8fl490c6BU25SE+Pp9gyRlmT/pc347LfEmAhU29HHi5XNnO4b4qL5mVy+NAdnQly4wzIRwhK/MbPYmekPzxRK21qyldKCUo42dKIKt19cyIKslHCHaSKMJX5jZqkz0x+6nC4KMwqpbR/gX/f+P/7+yr9ic/FKK6pmJs1+a4yZpc5Mf5iemE1tczZtHYvxDSyivKrciqqZkFiP35hZ6tTpGtIcS6lsdOH3CwVZ3eS6BqjprAt3aCbCTWQilkeAm4EWVV0VXPdN4EPAIHAc+JSqnh5j35NAN+ADvBOtFW2MgXRHMW/XJ5Od5mVB3mmSk7y4+236QxO6iXxWfBS4YdS6PcAqVS0F3gG+8h77X6WqayzpGzM+VaXLMwTAp8puICX9ODnZ1SQlDtr0h2bKjJv4VXUv0DFq3W5V9QYXXyUwkboxJgSn+wb5+YE6fhYsqrZ23mr+75ZtZKW4qOuqw5XsYsfGHTb9oQnZVIzxfxr4yXm2KbBbRBT4d1XdNQXnMyaq+P3Km7VuXjnejoiwaVnucFG10oJSS/RmyoWU+EXkHwAv8MR5mlyuqvUikgfsEZGq4CeIsY61HdgOUFRkY5gmNvQP+njmYD1NnR4uyE3l6pI80q2omplmk078IvIXBC76XqPnmbFdVeuD31tE5GlgPTBm4g9+GtgFgcnWJxuXMZHEmeAgw5nAuiIXy/LTrL6OmRGTuhFYRG4A7gVuUdW+87RJFZH0M6+B64Ajkw3UmGjR1Onhp6/X0u0ZQkT4YOlciq2SpplB4yZ+EXkSeAUoFpE6EdkGPAykExi+OSgi3w+2nScizwV3zQf+ICKHgH3A/6jq89PyLoyJAEM+P3vfaeWp12vo8gzRM+AdfydjpsG4Qz2qeucYq394nrYNwE3B1yeA1SFFZ0yUqO3oY09FM539Q5QWZnLZEiuqZsLHntw1ZgYcbehCxIqqmdnBEr8x0+R4aw8ZzgRy05PYXJxLnENIiLP6Oib87LfQmCnWN+jlubcaefZgAwdOuQFwJsRZ0jezhvX4jZkiqkpVUzf/+04rg14/H7gwm7JFWeEOy5hzWOI3ZopUNHax+2gzczOdXLsin+y0pHCHZMyYLPEbEwJVpXvAS4YzgeL8dFRhxdwMHA67J9/MXpb4jZmAsaZAXJBewouVgVs0P7lxEYnxDlbNzwx3qMaMyxK/MeMYPQViR5+bf9z9KKsyb2N+5lyuXPqnomrGRAJL/MaM48wUiK5kF16fg9b2ZXh6fdQmvMlXbthIWpL9NzKRxe4vM2YcNZ01ZDoDQzhxDj9JCV6KC3uJSz5oSd9EJEv8xowjO3EJh99NY9DrQAQWFbiR+CYWzrHy4SYyWeI35jwGvX5+93YL8QNX0OUZpK2nB7/6bQpEE/Hsc6oxY6hp72NPZTNd/UNcV1LM7WUZ/OrY08N39Wxbu81mxjIRyxK/MWOobOoiTuCOskIKXSlAHhcXWrFZEx0s8RsTdKylh8zkPxVVc4gVVTPRaUK/1SLyiIi0iMiREeuyRGSPiFQHv7vOs+/dwTbVInL3VAVuzFTpHfDyP4cb+dWhBt6oCRRVS4q3omomek30N/tR4IZR6+4DfqOqS4HfBJfPIiJZwP3ApQTm273/fH8gjJlpqkpFQxePv3KK4609XLYkhy3L88MdljHTbkKJX1X3Ah2jVt8KPBZ8/Rjw4TF2vR7Yo6odquoG9nDuHxBjwqKisYsXjjaRlZrAXRsWsn5xFnFWY8fEgFDG+PNVtTH4uonAHLujzQdqRyzXBdcZExaqSpfHS2ayFVUzsWtKBjFVVQEN5Rgisl1E9ovI/tbW1qkIy5izdPQO8rP9dfxsfy2DXj/xcYGiapb0TawJJfE3i8hcgOD3ljHa1AMLRiwXBtedQ1V3qWqZqpbl5uaGEJYxZ/P5lddPdvDEq6do7x1k44XZVlTNxLRQEv+zwJm7dO4GfjlGmxeA60TEFbyoe11wnTEzon/Qx1Ov1/CH6jYW56byyY0LWTkvExFL/CZ2TWiMX0SeBDYDOSJSR+BOnQeAn4rINuAU8GfBtmXA51T1M6raISL/DLwePNTXVHX0RWJjppyqIiI4ExxkpSSyflEWS/PTwx2WMbOCBIbnZ5eysjLdv39/uMMwEar+dD9732nl5tK5pDsTwh2OMTNCRA6oatlE2tqTuyZqDHr9vHy8jUO1p0l3JtA74LPEb8wYLPGbqHCqvZcXK1vo9gyxesEcLrswh8R4e/LWmLFY4jcRZay5b0sLSqlq6ibeIdxRtoD5c5LDHaYxs5olfhMxRs99e6ptkH/934f5+033sLl4JXEixFt9HWPGZf9LTMQ4M/dtWkI2p5py6HAvwj9YRHlVOUnxcZb0jZkg6/GbiHHqdA2psozKxjmoCvNyusjJHKSmsy7coRkTUSzxm4iR5ijhnXonOeleFuS5cSb6cPd3UpRpc98a837YZ2Mzq/n9Smf/EACfLrue1PRjZGdVk5gwZHPfGjNJlvjNrNXeM8DPDtQOF1VbO281X93yGbJSXNR11eFKdrFj4w6b+9aY98mGesys4/Mr+0928Nq7HSTEOdi0LHe4qFppQaklemNCZInfzCp9g17K36intXuAZfnpbC7OJTXJfk2NmUr2P8rMCmeKqiUnxJGTlsiGC7JZkpcW7rCMiUo2xm/Crs7dx5P7aun2DCEi3LBqriV9Y6aR9fhN2Ax4fbx8rI1DtZ1kJifQN2hF1YyZCZb4TVi829bLbyqb6RnwsrZoDh+womrGzBhL/CYsqpu7SYx38NHSBczNtKJqxsykSSd+ESkGfjJi1QXAP6nqd0a02UxgSsZ3g6vKVfVrkz2niVyqSnVLD3NSEshLd7KpONeKqhkTJpNO/Kr6NrAGQETiCEyi/vQYTX+vqjdP9jwm8vUMeHmpqoXjLT2snJfBdSsLSIqPC3dYxsSsqRrquQY4rqqnpuh4JgqoKkcbuthb3YrPp1y5LIe1C1zhDsuYmDdVif9jwJPn2bZRRA4BDcAOVT06ViMR2Q5sBygqsqJb0eBoQxd7KpopdCVz7Yp85qQkhjskYwxTMNm6iCQSSOorVbV51LYMwK+qPSJyE/CQqi4d75g22Xrk8vuVbo+XzJQEvD4/1S09lBSkIyLhDs2YqDbTk63fCLwxOukDqGrXiNfPici/iUiOqrZNwXlNmJxv+sO2ngFerAjcovnJjYtIjHewfG5GuMM1xowyFYn/Ts4zzCMiBUCzqqqIrCfwpHD7FJzThMno6Q/d/W6++cdvcWPRF2jrdJEY72Bz8Z+KqhljZp+QEr+IpALXAp8dse5zAKr6feB24PMi4gX6gY9pqGNLJqzOTH/oSg5cpE1LyObdhkJ+0n6Qz234EJuKc0lJtMdDjJnNQvofqqq9QPaodd8f8fph4OFQzmFml5rOGgozClEFEYiP85OVGk8/R7jxou3hDs8YMwH29Ix5X4oyi2g8PcA7tbkMDsUhApmZJ1lekBXu0IwxE2SJ30yYZ8jHgsQbebsui+4BD4M+bPpDYyKQDcaaCTnR2sNLVS30DLjYtv4qjvf+mrruUxRlFrFt7TabFcuYCGKJ30zIsZYekuId3FxaREHmMmB9uEMyxkySJX4zJlXlneYeXCkJ5GUEiqrFOxzEOew2TWMinY3xm3N0e4Z49lADz73VyMHa0wAkxcdZ0jcmSliP3wxTVY7UB4qqqSpXLstl7YI54Q7LGDPFLPGbYUcbunixspkFWSlsWZ5nRdWMiVKW+GOc3690eYaYk5LI8rkZJMQ5WJafZkXVjIlilvhjWGv3AC9WNtM7oqhacUF6uMMyxkwzS/wxyOvzs+9kB6+/68aZ4GBzcZ4VVTMmhljijzF9g15+caCOtp5Bls9NZ9OyPJITbRpEY2KJJf4YoaqICMkJceRlOLlsSQ4X5KaFOyxjTBjYffwxoLajjydeq6HLM4SIcP3KAkv6xsQw6/FHMc+Qj99Xt3GkvpM5KQl4Bn1kOBPCHZYxJsxCTvwichLoBnyAd/ScjxK4L/Ah4CagD/gLVX0j1POa80+BCHC8tYeXKlvoHfRStsjFhguySYizD3jGmKkb6rlKVdecZ6LfG4Glwa/twPcPZfbqAAALDklEQVSm6Jwx7cwUiO5+9/AUiDtf2cnhpsMAnGjtxZkYx53ri7hiaa4lfWPMsJnIBrcCj2vAq8AcEZk7A+eNaiOnQHSIgzlOFw7vAv7r4K8A2LQsl4+vLyI/wxnmSI0xs81UJH4FdovIAREZa+69+UDtiOW64LqziMh2EdkvIvtbW1unIKzoVtNZQ6YzE4DBoThONGbRcbqIysY+ABLjrZKmMWZsU3Fx93JVrReRPGCPiFSp6t73exBV3QXsAigrK7MJ2cdRlFlER58b/9B8GtoyUSAzo54L8uzirTHmvYXc41fV+uD3FuBpzp2hox5YMGK5MLjOhGBryVbqOpTqBifJSQMU5L6DJJ7kI8ttCkRjzHsLKfGLSKqIpJ95DVwHHBnV7FngkxKwAehU1cZQzhvL/H7F3TtIaUEp/3j1NkoK+0lMO0h+Rjo7Nu6wKRCNMeMKdagnH3g6WMkxHvixqj4vIp8DUNXvA88RuJXzGIHbOT8V4jljVku3hxcrWugbDBRVWztvNWvnrQ53WMaYCBNS4lfVE8A5mSeY8M+8VuALoZwn1nl9fva928HrJwNF1a4usaJqxpjJsyd3Z7m+QS8/P1BHe88gy+dmsGlZrhVVM8aExBL/LDWyqNrczGSuXJrLopzUcIdljIkC9jjnLHSqvZf/GlFU7doV+Zb0jTFTxnr8s4hnyMfed1o52tCFKyUBz5AVVTPGTD1L/LPEsZZuXqpqoX/Qz/rFWVy6OIt4q69jjJkGlvhniXfb+khJjOfDa/LJs/o6xphpZIk/TFSVysZuctISyctwsmlZLnEOsfo6xphpZ2MJYdDZP8QzB+t54WgTh+s6ASuqZoyZOdbjn0GqyqG6Tl4+1gbA5uJc1iyYE+aojDGxxhL/DDra0MVvq1pYmJ3CNcvzyUy2O3aMMTPPEv808/mVrv4hXKmJLJ+bQWK8g6V5aQTrGxljzIyzxB+i95r3tqXLw57KZvoGfNz9gUUkxjtYlp8e5oiNMbHOLu6G4Hzz3r7ZcIiXj7Xx5L5aege8XFWSS2K8/aiNMbOD9fhDMHLeWwBXsguvL44H9rzMhnlbWDkvgyuX5eJMsKJqxpjZwxJ/CGo6ayjMKARAFUQgKyWN2tZ6tq6bz8Jsq69jjJl9Jj3+ICILROS3IlIhIkdF5K/HaLNZRDpF5GDw659CC3d2KcosotPTSVdvEm/X5jIwFEfXQCfrFidY0jfGzFqhDDx7gb9V1RXABuALIrJijHa/V9U1wa+vhXC+WeeDSz5MdUMKR2tS8fuho68Lt8fN1hKb99YYM3tNeqgnOG9uY/B1t4hUAvOBiimKbVarbu7mzRPprMm+iXbfPgbiDjE3cwFbS2zeW2PM7DYlY/wisghYC7w2xuaNInIIaAB2qOrRqThnuJ1q7yM1KZ4vXXMJeelXhDscY4yZsJATv4ikAb8AvqSqXaM2vwEsVNUeEbkJeAZYep7jbAe2AxQVFYUa1pRTVY42dJGbnkR+hpMrl+US7xAcVl/HGBNhQrq5XEQSCCT9J1S1fPR2Ve1S1Z7g6+eABBHJGetYqrpLVctUtSw3NzeUsKZcZ98Q5W/Us6eimbdGFFWzpG+MiUST7vFLoObAD4FKVX3wPG0KgGZVVRFZT+APTftkzznT/H7lUN1pXj7WhohwdUkepYWZ4Q7LGGNCEspQz2XAJ4C3RORgcN3fA0UAqvp94Hbg8yLiBfqBj6mqhnDOGVXR2MXv3m5lcU4qVy/Ps2kQjTFRIZS7ev4AvOdYh6o+DDw82XOEg8+vdPYPkRUsquZMcHBhrhVVM8ZED3tyd4SWLg+7K5rpH/xTUbUleVZUzRgTXSzxA0M+P6+d6ODAKTcpiXFcVZJnRdWMMVEr5hN/74CXn+2vxd03xKr5mVyxNMeKqhljolrMJn5VRURISYyj0JXC1SXpFGWnhDssY4yZdjE5nvFuWy//+eopOvuHEBG2rMi3pG+MiRkx1ePvH/Txv++0UNnYTXZaIoNef7hDMsaYGRc1if+9pkAEeKe5m99WteAZ8nPpBVmsX5RFfFxMfuAxxsS4qMh855sC8XDT4eE2Ne19pDsT+PilRXzgwhxL+saYmBUVPf6xpkBUhf947Xn+YfMyCjKdbCrOJU6sqJoxxkRFt7ems4ZM559q6AwMxdHecSFv1QpHGwJF1RLirKiaMcZAlPT4izKLcPe7meN00Xo6lcaODDzeflYtUK4uyQt3eMYYM6tERY9/a8lW3B43J1u91LVl4IjrwJV1mO2X3mA1dowxZpSoSPylBaXs2LiDhbnxJKdXsGrhAPdd8SWbAtEYY8YQFUM9EEj+luiNMWZ8UdHjN8YYM3GhTr14g4i8LSLHROS+MbYnichPgttfC07KbowxJowmnfhFJA74LnAjsAK4U0RWjGq2DXCr6hLg28A3Jns+Y4wxUyOUHv964JiqnlDVQeAp4NZRbW4FHgu+/jlwjdhtNsYYE1ahJP75QO2I5brgujHbqKoX6ASyQzinMcaYEM2ai7sisl1E9ovI/tbW1nCHY4wxUSuUxF8PLBixXBhcN2YbEYkHMoH2sQ6mqrtUtUxVy3Jzc0MIyxhjzHsJJfG/DiwVkcUikgh8DHh2VJtngbuDr28HXlJVDeGcxhhjQiSh5GERuQn4DhAHPKKq/yIiXwP2q+qzIuIE/hNYC3QAH1PVExM4bitwapJh5QBtk9w3Utl7jn6x9n7B3vP7tVBVJzRcElLin41EZL+qloU7jplk7zn6xdr7BXvP02nWXNw1xhgzMyzxG2NMjInGxL8r3AGEgb3n6Bdr7xfsPU+bqBvjN8YY896iscdvjDHmPURN4h+vUmi0EZEFIvJbEakQkaMi8tfhjmmmiEiciLwpIv8d7lhmgojMEZGfi0iViFSKyMZwxzTdRORvgr/XR0TkyeCt4VFFRB4RkRYROTJiXZaI7BGR6uB313ScOyoS/wQrhUYbL/C3qroC2AB8IQbe8xl/DVSGO4gZ9BDwvKqWAKuJ8vcuIvOBLwJlqrqKwHNCHwtvVNPiUeCGUevuA36jqkuB3wSXp1xUJH4mVik0qqhqo6q+EXzdTSAZjC6SF3VEpBD4IPCDcMcyE0QkE7gS+CGAqg6q6unwRjUj4oHkYKmXFKAhzPFMOVXdS+DB1pFGVjR+DPjwdJw7WhL/RCqFRq3gBDdrgdfCG8mM+A5wL+APdyAzZDHQCvwoOLz1AxFJDXdQ00lV64GdQA3QCHSq6u7wRjVj8lW1Mfi6CcifjpNES+KPWSKSBvwC+JKqdoU7nukkIjcDLap6INyxzKB4YB3wPVVdC/QyTR//Z4vguPatBP7ozQNSReSu8EY184J1zabltstoSfwTqRQadUQkgUDSf0JVy8Mdzwy4DLhFRE4SGM67WkT+K7whTbs6oE5Vz3ya+zmBPwTRbAvwrqq2quoQUA58IMwxzZRmEZkLEPzeMh0niZbEP5FKoVElOJPZD4FKVX0w3PHMBFX9iqoWquoiAv/GL6lqVPcEVbUJqBWR4uCqa4CKMIY0E2qADSKSEvw9v4Yov6A9wsiKxncDv5yOk8RPx0Fnmqp6ReQe4AX+VCn0aJjDmm6XAZ8A3hKRg8F1f6+qz4UxJjM9/gp4ItipOQF8KszxTCtVfU1Efg68QeDutTeJwqd4ReRJYDOQIyJ1wP3AA8BPRWQbgQrFfzYt57Ynd40xJrZEy1CPMcaYCbLEb4wxMcYSvzHGxBhL/MYYE2Ms8RtjTIyxxG+MMTHGEr8xxsQYS/zGGBNj/j9F2qF/CnnBlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.clf() #clear figure\n",
    "\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predicted', alpha=.5)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model = LinearRegressionModel(1, 1)\n",
    "t_model.load_state_dict(torch.load('regression_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7068],\n",
       "        [ 2.7490],\n",
       "        [ 4.7912],\n",
       "        [ 6.8334],\n",
       "        [ 8.8757],\n",
       "        [10.9179],\n",
       "        [12.9601],\n",
       "        [15.0024],\n",
       "        [17.0446],\n",
       "        [19.0868],\n",
       "        [21.1291]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model(Variable(torch.from_numpy(x_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 146.0475311279297\n",
      "epoch 2, loss 74.51280975341797\n",
      "epoch 3, loss 6.099574565887451\n",
      "epoch 4, loss 0.5190826058387756\n",
      "epoch 5, loss 0.06365825980901718\n",
      "epoch 6, loss 0.026272492483258247\n",
      "epoch 7, loss 0.022987686097621918\n",
      "epoch 8, loss 0.022486984729766846\n",
      "epoch 9, loss 0.02221600152552128\n",
      "epoch 10, loss 0.0219662357121706\n",
      "epoch 11, loss 0.021720750257372856\n",
      "epoch 12, loss 0.021478276699781418\n",
      "epoch 13, loss 0.02123843878507614\n",
      "epoch 14, loss 0.02100123092532158\n",
      "epoch 15, loss 0.020766714587807655\n",
      "epoch 16, loss 0.020534764975309372\n",
      "epoch 17, loss 0.020305486395955086\n",
      "epoch 18, loss 0.020078780129551888\n",
      "epoch 19, loss 0.019854538142681122\n",
      "epoch 20, loss 0.019632818177342415\n",
      "epoch 21, loss 0.019413597881793976\n",
      "epoch 22, loss 0.01919674500823021\n",
      "epoch 23, loss 0.018982423469424248\n",
      "epoch 24, loss 0.018770460039377213\n",
      "epoch 25, loss 0.018560828641057014\n",
      "epoch 26, loss 0.018353551626205444\n",
      "epoch 27, loss 0.018148647621273994\n",
      "epoch 28, loss 0.01794600673019886\n",
      "epoch 29, loss 0.01774561032652855\n",
      "epoch 30, loss 0.017547406256198883\n",
      "epoch 31, loss 0.01735151931643486\n",
      "epoch 32, loss 0.017157703638076782\n",
      "epoch 33, loss 0.016966063529253006\n",
      "epoch 34, loss 0.016776684671640396\n",
      "epoch 35, loss 0.01658931002020836\n",
      "epoch 36, loss 0.016404064372181892\n",
      "epoch 37, loss 0.016220904886722565\n",
      "epoch 38, loss 0.016039742156863213\n",
      "epoch 39, loss 0.015860576182603836\n",
      "epoch 40, loss 0.015683509409427643\n",
      "epoch 41, loss 0.015508404932916164\n",
      "epoch 42, loss 0.015335222706198692\n",
      "epoch 43, loss 0.015163978561758995\n",
      "epoch 44, loss 0.014994610100984573\n",
      "epoch 45, loss 0.014827152714133263\n",
      "epoch 46, loss 0.014661579392850399\n",
      "epoch 47, loss 0.01449787337332964\n",
      "epoch 48, loss 0.01433595921844244\n",
      "epoch 49, loss 0.014175928197801113\n",
      "epoch 50, loss 0.014017531648278236\n",
      "epoch 51, loss 0.013861098326742649\n",
      "epoch 52, loss 0.013706299476325512\n",
      "epoch 53, loss 0.013553228229284286\n",
      "epoch 54, loss 0.013401862233877182\n",
      "epoch 55, loss 0.013252225704491138\n",
      "epoch 56, loss 0.013104235753417015\n",
      "epoch 57, loss 0.012957929633557796\n",
      "epoch 58, loss 0.012813213281333447\n",
      "epoch 59, loss 0.012670147232711315\n",
      "epoch 60, loss 0.012528649531304836\n",
      "epoch 61, loss 0.012388733215630054\n",
      "epoch 62, loss 0.012250450439751148\n",
      "epoch 63, loss 0.012113556265830994\n",
      "epoch 64, loss 0.011978340335190296\n",
      "epoch 65, loss 0.011844595894217491\n",
      "epoch 66, loss 0.011712273582816124\n",
      "epoch 67, loss 0.011581555008888245\n",
      "epoch 68, loss 0.011452222242951393\n",
      "epoch 69, loss 0.011324257589876652\n",
      "epoch 70, loss 0.011197865940630436\n",
      "epoch 71, loss 0.011072814464569092\n",
      "epoch 72, loss 0.010949171148240566\n",
      "epoch 73, loss 0.010826877318322659\n",
      "epoch 74, loss 0.010705993510782719\n",
      "epoch 75, loss 0.01058642566204071\n",
      "epoch 76, loss 0.010468228720128536\n",
      "epoch 77, loss 0.010351344011723995\n",
      "epoch 78, loss 0.010235696099698544\n",
      "epoch 79, loss 0.010121434926986694\n",
      "epoch 80, loss 0.010008427314460278\n",
      "epoch 81, loss 0.009896650910377502\n",
      "epoch 82, loss 0.009786102920770645\n",
      "epoch 83, loss 0.00967680849134922\n",
      "epoch 84, loss 0.009568803943693638\n",
      "epoch 85, loss 0.009461946785449982\n",
      "epoch 86, loss 0.009356283582746983\n",
      "epoch 87, loss 0.009251791052520275\n",
      "epoch 88, loss 0.009148458950221539\n",
      "epoch 89, loss 0.009046368300914764\n",
      "epoch 90, loss 0.008945300243794918\n",
      "epoch 91, loss 0.008845402859151363\n",
      "epoch 92, loss 0.008746640756726265\n",
      "epoch 93, loss 0.00864892452955246\n",
      "epoch 94, loss 0.008552390150725842\n",
      "epoch 95, loss 0.008456876501441002\n",
      "epoch 96, loss 0.00836245622485876\n",
      "epoch 97, loss 0.00826906319707632\n",
      "epoch 98, loss 0.00817667506635189\n",
      "epoch 99, loss 0.008085385896265507\n",
      "epoch 100, loss 0.007995115593075752\n",
      "Time 0:00:00.064657"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "epochs = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    e += 1\n",
    "    \n",
    "    # convert inputs/labels to variables\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward & get output\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(e, loss.data.item()))\n",
    "    \n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grace_python",
   "language": "python",
   "name": "grace_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
