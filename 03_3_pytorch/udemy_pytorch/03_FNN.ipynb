{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can represent linear functions but not non-linear functions\n",
    "\n",
    "![title](resource/logistic.png)\n",
    "![title](resource/nn.png)\n",
    "\n",
    "## Non-linear function\n",
    "- takes a number & perform mathematical operation\n",
    "    - ReLUs\n",
    "    - Sigmoid\n",
    "    - Tanh\n",
    "    \n",
    "#### Sigmoid (Logistic)\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Input number $\\rightarrow$ [0, 1]\n",
    "    - Large negative number $\\rightarrow$ 0\n",
    "    - Large positive number $\\rightarrow$ 1\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - Solution: Have to carefully initialize weights to prevent this\n",
    "    2. Outputs not centered around 0 \n",
    "        - If output always positive $\\rightarrow$ gradients always positive or negative $\\rightarrow$ **bad for gradient updates** \n",
    "\n",
    "#### Tanh\n",
    "- $\\tanh(x) = 2 \\sigma(2x) -1$\n",
    "    - A scaled sigmoid function\n",
    "- Input number $\\rightarrow$ [-1, 1]\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - **Solution**: Have to carefully initialize weights to prevent this\n",
    "\n",
    " \n",
    "#### ReLUs\n",
    "- $f(x) = \\max(0, x)$\n",
    "- Pros:\n",
    "    1. Accelerates convergence $\\rightarrow$ **train faster**\n",
    "    2. **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n",
    "- Cons:\n",
    "    1. Many ReLU units \"die\" $\\rightarrow$ **gradients = 0** forever\n",
    "        - **Solution**: careful learning rate choice\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Hidden layer Feedforward NN with sigmoid\n",
    "\n",
    "1. load dataset\n",
    "2. make dataset iterable\n",
    "3. create model & instantiate\n",
    "4. instantiate loss\n",
    "5. instantiate optimizer\n",
    "6. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedfowardNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_class):\n",
    "        super(FeedfowardNNModel, self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-linear\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Linear\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear\n",
    "        out = self.fc1(x)\n",
    "        # Non- Linear\n",
    "        out = self.sigmoid(out)\n",
    "        # linear\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input dimension = 28*28 = 784\n",
    "- ouput dimension = 10\n",
    "- hidden dimension : 100 (number of neurons, number of non-linear activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "hidden_dim = 100\n",
    "\n",
    "model = FeedfowardNNModel(input_dim ,hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss class for FNN : Cross Entropy Loss\n",
    "#### pytorch computes softmax and cross entropy loss simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f3c595aed58>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "print(len(list(model.parameters())))\n",
    "\n",
    "# hidden layer parameters\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC1 bias\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC2 parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 bias\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resource/dot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.6108832359313965, Accuracy 85\n",
      "Iteration : 1000,  Loss: 0.4910432696342468, Accuracy 89\n",
      "Iteration : 1500,  Loss: 0.3296413719654083, Accuracy 90\n",
      "Iteration : 2000,  Loss: 0.43148380517959595, Accuracy 91\n",
      "Iteration : 2500,  Loss: 0.28478577733039856, Accuracy 91\n",
      "Iteration : 3000,  Loss: 0.22945664823055267, Accuracy 92\n",
      "Time 0:00:35.293384"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedfowardNNModelWT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_class):\n",
    "        super(FeedfowardNNModelWT, self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-linear\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear\n",
    "        out = self.fc1(x)\n",
    "        # Non- Linear\n",
    "        out = self.tanh(out)\n",
    "        # linear\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.2955114245414734, Accuracy 91\n",
      "Iteration : 1000,  Loss: 0.16988371312618256, Accuracy 92\n",
      "Iteration : 1500,  Loss: 0.45927196741104126, Accuracy 93\n",
      "Iteration : 2000,  Loss: 0.18294626474380493, Accuracy 94\n",
      "Iteration : 2500,  Loss: 0.1142941266298294, Accuracy 94\n",
      "Iteration : 3000,  Loss: 0.11751607060432434, Accuracy 95\n",
      "Time 0:00:34.897965"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "model = FeedfowardNNModelWT(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedfowardNNModelWRe(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_class):\n",
    "        super(FeedfowardNNModelWRe, self).__init__()\n",
    "        #Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-linear\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear\n",
    "        out = self.fc1(x)\n",
    "        # Non- Linear\n",
    "        out = self.relu(out)\n",
    "        # linear\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.2781760096549988, Accuracy 91\n",
      "Iteration : 1000,  Loss: 0.1956309825181961, Accuracy 92\n",
      "Iteration : 1500,  Loss: 0.11842720955610275, Accuracy 93\n",
      "Iteration : 2000,  Loss: 0.35106608271598816, Accuracy 94\n",
      "Iteration : 2500,  Loss: 0.20275720953941345, Accuracy 95\n",
      "Iteration : 3000,  Loss: 0.13850967586040497, Accuracy 95\n",
      "Time 0:00:34.425624"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "model = FeedfowardNNModelWRe(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Hidden layer FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFeedforwardNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dime):\n",
    "        super(TFeedforwardNNModel, self).__init__()\n",
    "        # 784 -> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 100 -> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # 100 -> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        return self.fc3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.24669824540615082, Accuracy 91\n",
      "Iteration : 1000,  Loss: 0.35514146089553833, Accuracy 92\n",
      "Iteration : 1500,  Loss: 0.17534667253494263, Accuracy 94\n",
      "Iteration : 2000,  Loss: 0.12188761681318283, Accuracy 95\n",
      "Iteration : 2500,  Loss: 0.16846801340579987, Accuracy 96\n",
      "Iteration : 3000,  Loss: 0.16696247458457947, Accuracy 96\n",
      "Time 0:00:37.383510"
     ]
    }
   ],
   "source": [
    "model = TFeedforwardNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# train\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hidden layer FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThFeedforwardNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dime):\n",
    "        super(ThFeedforwardNNModel, self).__init__()\n",
    "        # 784 -> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 100 -> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # 100 -> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # 100 -> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        return self.fc4(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.3227856457233429, Accuracy 91\n",
      "Iteration : 1000,  Loss: 0.17028804123401642, Accuracy 93\n",
      "Iteration : 1500,  Loss: 0.091800756752491, Accuracy 95\n",
      "Iteration : 2000,  Loss: 0.09865974634885788, Accuracy 95\n",
      "Iteration : 2500,  Loss: 0.2647742033004761, Accuracy 96\n",
      "Iteration : 3000,  Loss: 0.08425090461969376, Accuracy 96\n",
      "Time 0:00:39.460579"
     ]
    }
   ],
   "source": [
    "model = ThFeedforwardNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# train\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "- 2 ways to expand NN\n",
    "    - more neurons (non-linear activation units)\n",
    "    - more hidden layers\n",
    "\n",
    "- cons : need larger datasets (curse of dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500,  Loss: 0.424159437417984, Accuracy 89\n",
      "Iteration : 1000,  Loss: 0.18057289719581604, Accuracy 94\n",
      "Iteration : 1500,  Loss: 0.2304164320230484, Accuracy 94\n",
      "Iteration : 2000,  Loss: 0.1671176701784134, Accuracy 95\n",
      "Iteration : 2500,  Loss: 0.11628420650959015, Accuracy 96\n",
      "Iteration : 3000,  Loss: 0.1326976865530014, Accuracy 96\n",
      "Time 0:00:41.997669"
     ]
    }
   ],
   "source": [
    "model = ThFeedforwardNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "iter = 0\n",
    "for e in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "#         images = images.view(-1, 28*28).cuda()\n",
    "#         labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu()==labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted==labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration : {},  Loss: {}, Accuracy {}'.format(iter, loss.data.item(), accuracy))\n",
    "\n",
    "sys.stdout.write('Time '+ str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grace_python",
   "language": "python",
   "name": "grace_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
