{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding in pytorch (remainder)\n",
    "\n",
    "- sparse representation : vocab 사이즈가 커지면 벡터의 차원도 같이 커진다. 고차원에 각 차원이 분리된 표현 방법\n",
    "- dense representation : 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤. 0/1이 아니라 실수값을 가진다. 저차원의 단어의 의미를 여러 차원에다가 분산하여 표현.\n",
    "    - word embedding : 단어를 dense vector로 표현. \n",
    "        - LSA, Word2Vec, FastText, GloVe\n",
    "        - Word2Vec (distributed representation): 단어의 의미를 다차원 공간에 벡터화. projection layer=1개의 shallow NN(활성화 함수 없음)\n",
    "            - CBOW : 주변에 있는 단어로 중간 단어를 예측.\n",
    "            - skip-gram : 중간에 있는 단어로 주변 단어를 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytroch의 nn.embedding()은 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 후에, 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습한다  \n",
    "1. input은 모두 encoding(integer)해야함\n",
    "- 단어 -> 단어의 고유한 정수 -> 임베딩 -> dense vector  \n",
    "임베딩은 정수에 대해 dense vector로 맵핑하고, dense vector는 train된다\n",
    "\n",
    "2. 단어 -> 정수 -> lookup table(train) -> 임베딩 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x11159bd30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': 2, 'how': 3, 'pytorch': 4, 'test': 5, 'and': 6, 'code': 7, 'I': 8, 'understand': 9, 'to': 10, 'want': 11, '<unk>': 0, '<pad>': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'I want to test pytorch embedding and understand how to code'\n",
    "word_set = set(sample.split(' '))\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>']= 0\n",
    "vocab['<pad>']=1\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding table\n",
    "embedding_t = torch.rand((len(vocab), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 11, 10,  5,  4,  2,  6,  9,  3, 10,  7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxes = []\n",
    "for w in sample.split(' '):\n",
    "    try:\n",
    "        idxes.append(vocab[w])\n",
    "    except KeyError:\n",
    "        idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes)\n",
    "idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4369, 0.5191, 0.6159],\n",
       "        [0.9351, 0.9412, 0.5995],\n",
       "        [0.3168, 0.6965, 0.9143],\n",
       "        [0.5932, 0.1123, 0.1535],\n",
       "        [0.5675, 0.8352, 0.2056],\n",
       "        [0.7932, 0.2783, 0.4820],\n",
       "        [0.2417, 0.7262, 0.7011],\n",
       "        [0.8102, 0.9801, 0.1147],\n",
       "        [0.8198, 0.9971, 0.6984],\n",
       "        [0.3168, 0.6965, 0.9143],\n",
       "        [0.2038, 0.6511, 0.7745]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 정수(단어)를 인덱스로 임베딩 테이블 값을 지정한다.\n",
    "lookup_r = embedding_t[idxes, :]\n",
    "lookup_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab),\n",
    "                              embedding_dim=3,\n",
    "                              padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-3.1700e-01, -1.0925e+00, -8.5194e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.9182e-04,  8.4189e-01,  1.8541e-01],\n",
       "        [ 6.2114e-01,  6.3818e-01, -2.4600e-01],\n",
       "        [ 2.3025e+00, -1.8817e+00, -4.9727e-02],\n",
       "        [-1.0450e+00,  7.9150e-01, -2.0252e-02],\n",
       "        [-4.3717e-01,  1.6459e+00, -2.4351e+00],\n",
       "        [-7.2915e-02, -3.3986e-02,  9.6252e-01],\n",
       "        [ 3.4917e-01, -9.2146e-01, -5.6195e-02],\n",
       "        [-7.0152e-01, -4.6372e-01,  1.9218e+00],\n",
       "        [-4.0255e-01,  1.2390e-01,  1.1648e+00],\n",
       "        [ 9.2337e-01,  1.3873e+00,  1.3750e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
